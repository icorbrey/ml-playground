{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Loss\n",
    "\n",
    "Iterative learning might remind you of the \"Hot and Cold\" kid's game for finding\n",
    "a hidden object like a thimble. In this game, the \"hidden object\" is the best\n",
    "possible model. You'll start with a wild guess (\"The value of $w_1$ is 0\") and\n",
    "wait for the system to tell you what the loss is. Then, you'll try another guess\n",
    "(\"The value of $w_1$ is 0.5\") and see what the loss is. Aah, you're getting\n",
    "warmer. Actually, if you play this game right, you'll usually be getting warmer.\n",
    "The real trick to the game is trying to find the best possible model as\n",
    "efficiently as possible.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Suppose we had the time and the computing resources to calculate the loss for\n",
    "all possible values of $w_1$. For the kind of regression problems we've been\n",
    "examining, the resulting plot of loss vs. $w_1$ will always be convex. In other\n",
    "words, the plot will always be bowl shaped.\n",
    "\n",
    "Convex problems have only one minimum; that is, only one place where the slope\n",
    "is exactly 0. That minimum is where the loss function converges. Calculating the\n",
    "loss function for ever conceivable value of $w_1$ over the entire data set would\n",
    "be an inefficient way of finding the convergence point. Let's examine a better\n",
    "mechanism - very popular in machine learning - called **gradient descent**.\n",
    "\n",
    "The first stage in gradient descent is to pick a starting value (a starting\n",
    "point) for $w_1$. The starting point doesn't matter much; therefore, many\n",
    "algorithms simply set $w_1$ to 0 or pick a random value.\n",
    "\n",
    "The gradient descent algorithm then calculates the gradient of the loss curve at\n",
    "the starting point. The gradient of the loss is equal to the derivative (or the\n",
    "slope) of the curve, and tells you which way is \"warmer\" or \"colder\". When there\n",
    "are multiple weights, the **gradient** is a vector of partial derivatives with\n",
    "respect to the weights.\n",
    "\n",
    "Note that a gradient is a vector, so it has both direction and magnitude. The\n",
    "gradient always points in the direction of steepest increase in the loss\n",
    "function. The gradient descent algorithm takes a step in the direction of the\n",
    "negative gradient in order to reduce loss as quickly as possible.\n",
    "\n",
    "To determine the next point along the loss function curve, the gradient descent\n",
    "algorithm adds some fraction of the gradient's magnitude to the starting point.\n",
    "The gradient descent then repeats this process, edging ever closer to the\n",
    "minimum.\n",
    "\n",
    "## Learning Rate\n",
    "\n",
    "As noted, the gradient vector has both direction and magnitude. Gradient descent\n",
    "algorithms multiply the gradient by a scalar known as the **learning rate** \n",
    "(also sometimes called the **step size**) to determine the next point. For\n",
    "example, if the gradient magnitude is 2.5 and the learning rate is 0.01, then\n",
    "the gradient descent algorithm will pick the next point 0.025 units away from\n",
    "the previous point.\n",
    "\n",
    "**Hyperparameters** are the knobs that programmers tweak in machine learning\n",
    "algorithms. Most machine learning programmers spend a fair amount of time tuning\n",
    "the learning rate. If you pick a learning rate that is too small, learning will\n",
    "take too long. Conversely, if you specify a learning rate that is too large, the\n",
    "next point will perpetually bounce haphazardly across the bottom of the well\n",
    "like a quantum mechanics experiment gone horribly wrong.\n",
    "\n",
    "There's a Goldilocks learning rate for every regression problem. The Goldilocks\n",
    "value is related to how flat the loss function is. If you know the gradient of\n",
    "the loss function is small then you can safely try a larger learning rate, which\n",
    "compensates for the small gradient and results in a larger step size.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "In gradient descent, a **batch** is the total number of examples you use to\n",
    "calculate the gradient in a single iteration. So far, we've assumed that the\n",
    "batch has been the entire data set. When looking at Google scale, data sets\n",
    "often contain billions or even hundreds of billions of examples. Furthermore,\n",
    "Google data sets often contain huge numbers of features. Consequently, a batch\n",
    "can be enormous. A very large batch may cause even a single iteration to take a\n",
    "very long time to compute.\n",
    "\n",
    "A large data set with randomly sampled examples probably contains redundant\n",
    "data. In fact, redundancy becomes more likely as the batch size grows. Some\n",
    "redundancy can be useful to smooth out noisy gradients, but enormous batches\n",
    "tend not to carry much more predictive value than large batches.\n",
    "\n",
    "What if we could get the right gradient _on average_ for much less computation?\n",
    "By choosing examples at random from our data set, we could estimate (albiet,\n",
    "noisily) a big average from a much smaller one. **Stochastic gradient descent**\n",
    "**(SGD)** takes this idea to the extreme - it uses only a single example (a\n",
    "batch size of 1) per iteration. Given enough iterations, SGD works but is very\n",
    "noisy. The term \"stochastic\" indicates that the one example comprising each\n",
    "batch is chosen at random.\n",
    "\n",
    "**Mini-batch stochastic gradient descent (mini-batch SGD)** is a compromise\n",
    "between full-batch iteration and SGD. A mini-batch is typically between 10 and\n",
    "1,000 examples, chosen at random. Mini-batch SGD reduces the amount of noise in\n",
    "SGD but is still more efficient than full-batch."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
